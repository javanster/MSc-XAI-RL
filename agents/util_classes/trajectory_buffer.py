import numpy as np
import scipy


class TrajectoryBuffer:
    """
    A buffer for storing and processing trajectories in Proximal Policy Optimization (PPO).

    The TrajectoryBuffer class is designed to collect and manage trajectories, which are sequences of
    states, actions, rewards, value estimates, and log probabilities generated by an agent interacting
    with an environment. It facilitates the computation of advantage estimates and returns, essential
    for training the policy (actor) and value (critic) networks in PPO. The buffer ensures efficient
    data storage and retrieval, supporting both discrete and continuous action spaces.

    Parameters
    ----------
    observation_dimensions : int
        The dimensionality of the observation (state) space.
    action_dimensions : int
        The dimensionality of the action space. For discrete action spaces, this is typically 1.
        For continuous action spaces, this corresponds to the number of action dimensions.
    size : int
        The maximum number of time steps (buffer size) the TrajectoryBuffer can hold.
    gamma : float, optional
        The discount factor for future rewards, by default 0.99.
    lam : float, optional
        The lambda parameter for Generalized Advantage Estimation (GAE), by default 0.95.

    Attributes
    ----------
    size : int
        The maximum number of time steps the buffer can hold.
    observation_buffer : np.ndarray
        Preallocated buffer for storing observations (states). Shape: (size, observation_dimensions).
    action_buffer : np.ndarray
        Preallocated buffer for storing actions taken. Shape: (size, action_dimensions).
    advantage_buffer : np.ndarray
        Buffer for storing advantage estimates. Shape: (size,).
    reward_buffer : np.ndarray
        Buffer for storing rewards received. Shape: (size,).
    return_buffer : np.ndarray
        Buffer for storing computed returns (discounted sum of rewards). Shape: (size,).
    value_buffer : np.ndarray
        Buffer for storing value estimates from the critic network. Shape: (size,).
    logprob_buffer : np.ndarray
        Buffer for storing log probabilities of actions under the current policy. Shape: (size,).
    gamma : float
        The discount factor for future rewards.
    lam : float
        The lambda parameter for GAE.
    pointer : int
        Current index in the buffer for inserting new data.
    trajectory_start_index : int
        Index marking the start of the current trajectory in the buffer.

    Methods
    -------
    _discounted_cumulative_sums(sequence, discount)
        Compute discounted cumulative sums of a sequence.
    insert(observation, action, reward, value, logprob)
        Insert a single time step's data into the trajectory buffer.
    complete_episode_trajectory(last_value=0)
        Finalize the current episode trajectory by computing advantage estimates and returns.
    get_and_reset()
        Retrieve and reset the trajectory buffer after ensuring it is full.

    Notes
    -----
    - The buffer is designed to handle both discrete and continuous action spaces by accepting
      multi-dimensional action arrays.
    - After collecting a full trajectory, use `complete_episode_trajectory` to process the data
      before retrieving it with `get_and_reset`.
    - The buffer must be fully filled before calling `get_and_reset`, otherwise a ValueError is raised.
    - Advantages are normalized within `get_and_reset` to stabilize training.
    """

    def __init__(
        self,
        observation_dimensions: int,
        action_dimensions: int,
        size: int,
        gamma: float = 0.99,
        lam: float = 0.95,
    ) -> None:
        self.size = size
        self.observation_buffer: np.ndarray = np.zeros(
            shape=(size, observation_dimensions), dtype=np.float32
        )
        self.action_buffer: np.ndarray = np.zeros(shape=(size, action_dimensions), dtype=np.float32)
        self.advantage_buffer: np.ndarray = np.zeros(shape=size, dtype=np.float32)
        self.reward_buffer: np.ndarray = np.zeros(shape=size, dtype=np.float32)
        self.return_buffer: np.ndarray = np.zeros(shape=size, dtype=np.float32)
        self.value_buffer: np.ndarray = np.zeros(shape=size, dtype=np.float32)
        self.logprob_buffer: np.ndarray = np.zeros(shape=size, dtype=np.float32)
        self.gamma: float = gamma
        self.lam: float = lam
        self.pointer: int = 0
        self.trajectory_start_index: int = 0

    def _discounted_cumulative_sums(self, sequence: np.ndarray, discount: float):
        """
        Compute discounted cumulative sums of a sequence.

        This method calculates the discounted cumulative sums of the input sequence using the specified
        discount factor. Used for computing rewards-to-go and advantage estimates in Proximal Policy
        Optimization (PPO).

        Parameters
        ----------
        sequence : np.ndarray
            A one-dimensional or multi-dimensional NumPy array representing the sequence of values
            (e.g., rewards or temporal difference residuals) to be cumulatively summed with discounting.

        discount : float
            The discount factor applied to future elements in the sequence. It should be a value
            between 0 and 1, where a higher value places more emphasis on future rewards.

        Returns
        -------
        np.ndarray
            A NumPy array of the same shape as `sequence` containing the discounted cumulative sums.
            Each element in the returned array represents the sum of the current and all future elements
            in `sequence`, each multiplied by the discount factor raised to the power of their respective
            time step difference.

        Raises
        ------
        ValueError
            If `discount` is not a float between 0 and 1.

        Notes
        -----
        - The method reverses the input `sequence`, applies a linear filter to compute the cumulative sum,
          and then reverses the result to obtain the final discounted sums.
        - This implementation leverages `scipy.signal.lfilter` for efficient computation without explicit loops.
        - Ensure that the input `sequence` does not contain NaN or infinite values to prevent unexpected behavior.
        """
        if not not (0.0 <= discount <= 1.0):
            raise ValueError('"discount" must be a float between 0 and 1.')

        return scipy.signal.lfilter([1], [1, float(-discount)], sequence[::-1], axis=0)[::-1]

    def insert(
        self,
        observation: np.ndarray,
        action: np.ndarray,
        reward: float,
        value: float,
        logprob: float,
    ) -> None:
        """
        Insert a single time step's data into the trajectory buffer.

        This method stores the provided observation, action, reward, value estimate, and log probability
        of the action into their respective buffers at the current pointer position. After insertion,
        the pointer is incremented to prepare for the next data entry. If the buffer has reached its
        maximum capacity, a `ValueError` is raised to prevent buffer overflow.

        Parameters
        ----------
        observation : np.ndarray
            The observation (state) at the current time step. The shape should match the
            `observation_dimensions` specified during buffer initialization.

        action : np.ndarray
            The action taken by the agent at the current time step. This should be a one-dimensional
            NumPy array of type `float32`. For discrete action spaces, represent the action as a single-element
            array containing the action index as a float (e.g., `np.array([action_index], dtype=np.float32)`).
            For continuous action spaces, represent the action as a multi-element array containing the
            action values (e.g., `np.array([dim1, dim2, ...], dtype=np.float32)`).

        reward : float
            The reward received after taking the action.

        value : float
            The estimated value of the current state, as predicted by the value function (critic).

        logprob : float
            The log probability of the action under the current policy, used for computing the PPO
            objective.

        Raises
        ------
        ValueError
            If the buffer has reached its maximum capacity (`self.size`) and cannot accept more data.
        """
        if self.pointer >= self.size:
            raise ValueError(
                "Buffer overflow. Consider increasing buffer size or handling buffer reset."
            )

        self.observation_buffer[self.pointer] = observation
        self.action_buffer[self.pointer] = action
        self.reward_buffer[self.pointer] = reward
        self.value_buffer[self.pointer] = value
        self.logprob_buffer[self.pointer] = logprob
        self.pointer += 1

    def complete_episode_trajectory(self, last_value: float = 0) -> None:
        """
        Finalize the current episode trajectory (all steps taken in an episode) by computing advantage
        estimates and returns.

        This method processes the trajectory data collected from `trajectory_start_index` up to the
        current `pointer`. It calculates the advantage estimates using Generalized Advantage Estimation (GAE)
        and computes the discounted returns (rewards-to-go) for the trajectory. After processing, it updates
        the advantage and return buffers and resets the `trajectory_start_index` for the next trajectory.

        Parameters
        ----------
        last_value : float, optional
            The estimated value of the state following the last state in the trajectory. This is used for
            bootstrapping the return and advantage calculations. Default is 0, which is appropriate if
            the trajectory ends in a terminal state.

        Notes
        -----
        - Assumes that the buffer contains a complete trajectory from `trajectory_start_index` to `pointer`.
        - The advantage estimates are normalized later when retrieving data from the buffer.
        - After calling this method, the `trajectory_start_index` is set to the current `pointer`, preparing
          the buffer for the next trajectory.
        """
        path_slice = slice(self.trajectory_start_index, self.pointer)
        rewards = np.append(self.reward_buffer[path_slice], last_value)
        values = np.append(self.value_buffer[path_slice], last_value)

        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]

        self.advantage_buffer[path_slice] = self._discounted_cumulative_sums(
            deltas, self.gamma * self.lam
        )
        self.return_buffer[path_slice] = self._discounted_cumulative_sums(rewards, self.gamma)[:-1]

        self.trajectory_start_index = self.pointer

    def get_and_reset(self):
        """
        Retrieve and reset the trajectory buffer after ensuring it is full.

        This method returns all the data stored in the buffer, normalizes the advantage estimates,
        and then resets the buffer for the next collection phase. It raises an error if called
        before the buffer is completely filled, ensuring that only complete and valid data batches
        are used for training.

        Raises
        ------
        ValueError
            If the buffer is not yet full (i.e., `pointer` does not equal `size`).

        Returns
        -------
        tuple
            A tuple containing the following NumPy arrays:

            - **observation_buffer** (`np.ndarray`):
              Array of observations with shape `(size, observation_dimensions)`.

            - **action_buffer** (`np.ndarray`):
              Array of actions taken with shape `(size,)`.

            - **advantage_buffer** (`np.ndarray`):
              Normalized advantage estimates with shape `(size,)`.

            - **return_buffer** (`np.ndarray`):
              Array of computed returns (discounted sum of rewards) with shape `(size,)`.

            - **logprob_buffer** (`np.ndarray`):
              Array of log probabilities of the actions under the current policy with shape `(size,)`.

        Notes
        -----
        - Ensure that the buffer is fully populated by inserting exactly `size` elements before calling this method.
        - Normalization of the advantage estimates is performed to stabilize training by having zero mean and unit variance.
        - After retrieval, the buffer is reset, and new data can be collected for the next training iteration.
        """
        if self.pointer != self.size:
            raise ValueError("get_and_empty called before buffer was full")

        advantage_mean = np.mean(self.advantage_buffer)
        advantage_std = np.std(self.advantage_buffer)
        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std

        self.pointer, self.trajectory_start_index = 0, 0

        return (
            self.observation_buffer,
            self.action_buffer,
            self.advantage_buffer,
            self.return_buffer,
            self.logprob_buffer,
        )
