import numpy as np
import scipy


class TrajectoryBuffer:
    """
    A buffer for storing and processing discrete-action trajectories in Proximal Policy Optimization (PPO).

    The `TrajectoryBuffer` class is tailored for discrete action spaces, where each action is
    represented by an integer. It collects and manages trajectories consisting of states,
    actions, rewards, value estimates, and log probabilities generated by an agent interacting
    with an environment. It also facilitates the computation of advantage estimates and returns,
    which are essential for training actor and critic networks in PPO.

    Parameters
    ----------
    observation_dimensions : int
        The dimensionality of the observation (state) space.
    size : int
        The maximum number of time steps (buffer size) the TrajectoryBuffer can hold.
    gamma : float, optional
        The discount factor for future rewards, by default 0.99.
    lam : float, optional
        The lambda parameter for Generalized Advantage Estimation (GAE), by default 0.95.

    Attributes
    ----------
    size : int
        The maximum number of time steps the buffer can hold.
    observation_buffer : np.ndarray
        Preallocated buffer for storing observations (states).
        Shape: (size, observation_dimensions).
    action_buffer : np.ndarray
        Preallocated buffer for storing discrete actions taken.
        Shape: (size,).
    advantage_buffer : np.ndarray
        Buffer for storing advantage estimates.
        Shape: (size,).
    reward_buffer : np.ndarray
        Buffer for storing rewards received.
        Shape: (size,).
    return_buffer : np.ndarray
        Buffer for storing computed returns (discounted sum of rewards).
        Shape: (size,).
    value_buffer : np.ndarray
        Buffer for storing value estimates from the critic network.
        Shape: (size,).
    logprob_buffer : np.ndarray
        Buffer for storing log probabilities of actions under the current policy.
        Shape: (size,).
    gamma : float
        The discount factor for future rewards.
    lam : float
        The lambda parameter for GAE.
    pointer : int
        Current index in the buffer for inserting new data.
    trajectory_start_index : int
        Index marking the start of the current trajectory in the buffer.

    Methods
    -------
    _discounted_cumulative_sums(sequence, discount)
        Compute discounted cumulative sums of a sequence.
    insert(observation, action, reward, value, logprob)
        Insert a single time step's data into the trajectory buffer.
    complete_episode_trajectory(last_value=0)
        Finalize the current episode trajectory by computing advantage estimates and returns.
    get_and_reset()
        Retrieve and reset the trajectory buffer after ensuring it is full.

    Notes
    -----
    - This buffer is specifically designed for discrete action spaces, where each action is
      an integer.
    - After collecting a full trajectory or multiple trajectories, use
      `complete_episode_trajectory` to process the data before retrieving it with
      `get_and_reset`.
    - The buffer must be fully filled before calling `get_and_reset`, otherwise a
      ValueError is raised.
    - Advantages are normalized within `get_and_reset` to stabilize training.
    """

    def __init__(
        self,
        observation_dimensions: int,
        size: int,
        gamma: float = 0.99,
        lam: float = 0.95,
    ) -> None:
        self.size = size
        self.observation_buffer: np.ndarray = np.zeros(
            shape=(size, observation_dimensions), dtype=np.float32
        )
        self.action_buffer: np.ndarray = np.zeros(size, dtype=np.int32)
        self.advantage_buffer: np.ndarray = np.zeros(shape=size, dtype=np.float32)
        self.reward_buffer: np.ndarray = np.zeros(shape=size, dtype=np.float32)
        self.return_buffer: np.ndarray = np.zeros(shape=size, dtype=np.float32)
        self.value_buffer: np.ndarray = np.zeros(shape=size, dtype=np.float32)
        self.logprob_buffer: np.ndarray = np.zeros(shape=size, dtype=np.float32)
        self.gamma: float = gamma
        self.lam: float = lam
        self.pointer: int = 0
        self.trajectory_start_index: int = 0

    def _discounted_cumulative_sums(self, sequence: np.ndarray, discount: float):
        """
        Compute discounted cumulative sums of a sequence.

        This method calculates the discounted cumulative sums of the input sequence using
        the specified discount factor. It is used to compute rewards-to-go and advantage
        estimates in Proximal Policy Optimization (PPO).

        Parameters
        ----------
        sequence : np.ndarray
            A one-dimensional NumPy array representing the sequence of values (e.g., rewards
            or temporal-difference residuals) to be cumulatively summed with discounting.
        discount : float
            The discount factor applied to future elements in the sequence. Must be between 0 and 1,
            where a higher value places more emphasis on future rewards.

        Returns
        -------
        np.ndarray
            A NumPy array of the same shape as `sequence` containing the discounted cumulative sums.
            Each element represents the sum of the current and all future elements in `sequence`,
            each multiplied by `discount` raised to the power of their respective time step difference.

        Raises
        ------
        ValueError
            If `discount` is not a float between 0 and 1.

        Notes
        -----
        - The method reverses the input `sequence`, applies a linear filter to compute the
          cumulative sum, and then reverses the result to obtain the final discounted sums.
        - This implementation uses `scipy.signal.lfilter` for efficient computation.
        """
        if not (0.0 <= discount <= 1.0):
            raise ValueError('"discount" must be a float between 0 and 1.')

        return scipy.signal.lfilter([1], [1, float(-discount)], sequence[::-1], axis=0)[::-1]

    def insert(
        self,
        observation: np.ndarray,
        action: int,
        reward: float,
        value: float,
        logprob: float,
    ) -> None:
        """
        Insert a single time step's data into the trajectory buffer.

        Parameters
        ----------
        observation : np.ndarray
            The observation (state) at the current time step. The shape should match
            `observation_dimensions` specified during buffer initialization.
        action : int
            The discrete action taken at the current time step (integer).
        reward : float
            The reward received after taking the action.
        value : float
            The estimated value of the current state, as predicted by the value function.
        logprob : float
            The log probability of the action under the current policy, used for computing
            the PPO objective.

        Raises
        ------
        ValueError
            If the buffer has reached its maximum capacity (`self.size`).
        """
        if self.pointer >= self.size:
            raise ValueError(
                "Buffer overflow. Consider increasing buffer size or handling buffer reset."
            )

        self.observation_buffer[self.pointer] = observation
        self.action_buffer[self.pointer] = action
        self.reward_buffer[self.pointer] = reward
        self.value_buffer[self.pointer] = value
        self.logprob_buffer[self.pointer] = logprob
        self.pointer += 1

    def complete_episode_trajectory(self, last_value: float = 0) -> None:
        """
        Finalize the current episode trajectory by computing advantage estimates and returns.

        This method processes the trajectory from `trajectory_start_index` up to `pointer`.
        It calculates advantage estimates using Generalized Advantage Estimation (GAE)
        and computes discounted returns (rewards-to-go).

        Parameters
        ----------
        last_value : float, optional
            The estimated value of the state following the last state in the trajectory,
            used for bootstrapping. Defaults to 0, which is appropriate if the episode
            ends in a terminal state.

        Notes
        -----
        - This assumes that the data from `trajectory_start_index` to `pointer` represents
          a complete episode.
        - After calling this method, `trajectory_start_index` is set to `pointer`.
        """
        path_slice = slice(self.trajectory_start_index, self.pointer)
        rewards = np.append(self.reward_buffer[path_slice], last_value)
        values = np.append(self.value_buffer[path_slice], last_value)

        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]

        self.advantage_buffer[path_slice] = self._discounted_cumulative_sums(
            deltas, self.gamma * self.lam
        )
        self.return_buffer[path_slice] = self._discounted_cumulative_sums(rewards, self.gamma)[:-1]

        self.trajectory_start_index = self.pointer

    def get_and_reset(self):
        """
        Retrieve and reset the trajectory buffer after ensuring it is full.

        This method returns all stored data, normalizes the advantage estimates,
        and then resets the buffer for the next data collection phase.

        Returns
        -------
        tuple
            A tuple containing:
            - observation_buffer (np.ndarray): Observations, shape (size, observation_dimensions).
            - action_buffer (np.ndarray): Discrete actions taken, shape (size,).
            - advantage_buffer (np.ndarray): Normalized advantage estimates, shape (size,).
            - return_buffer (np.ndarray): Computed returns (discounted sum of rewards), shape (size,).
            - logprob_buffer (np.ndarray): Log probabilities of actions, shape (size,).

        Raises
        ------
        ValueError
            If the buffer is not yet full (i.e., `pointer` != `size`).

        Notes
        -----
        - Call `complete_episode_trajectory` before `get_and_reset` to ensure
          advantage estimates and returns are correctly calculated.
        - The advantage estimates are normalized to stabilize training.
        - After retrieval, the buffer is cleared and can be reused for subsequent data collection.
        """
        if self.pointer != self.size:
            raise ValueError("get_and_empty called before buffer was full")

        advantage_mean = np.mean(self.advantage_buffer)
        advantage_std = np.std(self.advantage_buffer)
        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std

        self.pointer, self.trajectory_start_index = 0, 0

        return (
            self.observation_buffer,
            self.action_buffer,
            self.advantage_buffer,
            self.return_buffer,
            self.logprob_buffer,
        )
